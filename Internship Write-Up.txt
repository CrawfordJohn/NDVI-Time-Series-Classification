NDVI-Time-Series-Classification

Electricity is an essential part of modern-day society. People use electricity for almost everything, washing their laundry, cooking their food, and heating and lighting up
their homes. Because of this, when power outages occur, they can cause plenty of problems for the people affected. Collapsing trees during storms can crash down onto the power
lines and conduct all the electricity away from them, causing a power outage. These power outages usually occur due to these taller trees being too close to the lines, so it 
can be useful to determine possible risky areas where this could occur so communities can prepare and prevent this from happening. 
Being able to determine the type of trees that are near power lines can be valuable information since each type of tree has a different risk level. Trees can be classified into
two main groups, deciduous trees (trees that lose their leaves in the winter) and evergreen trees. When observing trees in person, it may seem relatively easy to distinguish 
between the needle-shaped evergreen leaves and the distinct wide flat leaves that are found on deciduous trees. This distinction is made even easier during the winter and fall 
months when the leaves on the deciduous trees turn a variety of colors or simply fall off. While this classification process may seem easy to do in person, it is a grueling and 
repetitive task to visit every tree in a specific area to classify it by hand. Using satellite data, this laborious process can be avoided by automating a way to differentiate 
between these two main types of trees.

The Normalized Difference Vegetation Index (NDVI) is a measurement of the vegetation (greenery) in a specific area using satellites. The satellites collect information on the 
near-infrared light and the red light in certain areas and take the difference in order to calculate the NDVI. Vegetation reflects this near-infrared light and it absorbs red 
light, so if there’s more near-infrared light and less red light, there will be a greater NDVI value. NDVI values are standardized between the values of -1 and 1, with each 
value allowing one to assume a different scene classification (water, vegetated, not-vegetated, cloud-cover). Using satellite data, a time series can be created using this 
information at specific locations to observe the changes in vegetation over the whole year. Since deciduous trees lose their leaves during the winter months, they register 
lower on the NDVI index. A time series that plots the NDVI of a deciduous tree over the span of a year would have a spike in the spring and a steep drop-off in the fall. 
Evergreen trees don’t lose their leaves, so they have a relatively static NDVI value all year round. With a time series classification model, groups of these different 
deciduous and evergreen time series can be used to train a model that can predict the type of tree just based on the shape of the time series. 

Before inputting the data into the model, it is crucial to format it correctly. This requires a thorough understanding of our raw data. The raw data used for this model was 
given in a time series with a frequency of 5 days. It contains data from four different patches of deciduous trees in Texas. The time series includes NDVI index data for these 
selected areas of longitude and latitude from May 2018 to June 2021. The data comes with a timestamp of UNIX time in milliseconds, which can easily be converted into a DateTime 
format using Pandas. Those specific dates can then be used to create a day of year column to allow the NDVI values to be observed as a seasonal yearly trend. Along with each 
NDVI value, another time series can be created with scene classification values for the same locations, allowing us to easily filter out any cloud-covered days where the 
satellite cannot detect the NDVI. 

The first step in preparing this time series for any time series model is to find and drop any outliers observed in the data. The first part of this step is pretty easy, just 
filter the scene classifications to values of 4 or 5 (vegetated or not vegetated). However, this scene classification method is not 100% accurate, and there can still be some 
outliers that need to be cleaned up. Since the time series for deciduous trees comes in a bimodal fashion, the mean and standard deviation of the entire time series cannot be 
used to determine outliers. A rolling window method needs to be used, where the mean and standard deviation are calculated based on a specific value of previous data points. 
Using a baseline of 2 standard deviations away from the mean to determine outliers, a rolling window of 4 data points seemed to be adequate for removing outliers, producing a 
natural-looking time series when shown in a graph. After these outliers have been removed from the time series, there is another problem that arises. Since the outlier 
detection deleted some of the points at the ends of each time series, they no longer have the same length of rows. For the model chosen to train and classify these time series 
(specific model description given later), each time series must have the same shape, so the time series needed to be cut down to the minimum of each file’s maximum index value. 
One problem that could arise from this, however, is that with a large amount of different time series datasets this minimum value solution could cut out a significant amount of 
data as it is harder to find a common max index value. However, this is not a problem for our lowly four-time series files. The other missing values in between the first and 
last values don’t matter for creating same-shape data frames, as these data points can be repainted with simple interpolation. 

The indices from the original data can be cycled through to create a list of where the outliers were removed and need to be interpolated for. Once this list is created, the 
SciPy function ‘interp1d’ can be used to take a simple interpolation between the previous and next point for those missing values. These interpolated values can then be placed 
into a data frame to combine with the outlier-removed data frame to complete the reconstructed time series. In order to distinguish the interpolated values from the actual 
data, the scene classification value for these interpolated values was simply set to the string “Interpolated” instead of a classification number. 

Now that our time series has outliers removed and replaced by interpolated values, it can now be inputted into a time series classifier model. For this situation, Rocket 
transform was used from the Sktime package with a RidgeClassifier. Rocket uses a set number of random kernels to create different feature maps for each different time series 
classification. (more info: https://www.sktime.org/en/stable/examples/rocket.html) These features are then passed to train the RidgeClassifier with logistic regression. This 
ridge classifier works by converting the target values to -1 and 1 and using the inputted regression to determine which class the time series belongs to (more info: 
https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html). In order to prepare Rocket to train the RidgeClassifier, all of the time series 
need to be formatted so that the entire time series is an array with each array being one cell in a two-column data frame. Each of the time series’ classifications (deciduous, 
evergreen) should also be placed into a separate NumPy array, with the index lined up with its corresponding time series. Since all of the provided data was time series for 
deciduous trees, it was necessary to create a simulated evergreen tree time series. This was accomplished by setting a static value and adding some random noise. For this model 
training, 80% of the data was used for training and the other 20% for testing. Since there were only 4 deciduous data frames,  3 deciduous and 3 evergreen time series were used 
for training, and 1 of each category was used for testing. 

With the three training deciduous tree time series and the randomly generated sample evergreen time series, the model was able to correctly identify the two test time series. 
It would be ideal to have more data to truly assess the model accuracy with some actual evergreen time series, but with what the model was given, it performed well. Even when 
the model received only half of the time series for training, it still classified all the test time series correctly. This did raise an alarm that there could be some other 
factor that was causing the model to always give out the correct answer, but when the train data frame was reduced to one of each type, the model faltered and didn’t predict 
the classifications correctly. 

This model still has future room for improvement and testing. First off, it needs more data to properly quantify its accuracy and increase the model’s strength in general. 
There are only so many conclusions that can be derived from a model that intakes a sample size of less than five. Also, it would be necessary to find time series for deciduous 
trees outside of Texas, because unless Texas is the only population of interest, there could be underlying variables in Texas that cause these time series to look similar. The 
time series for a group of deciduous trees in New Hampshire may look different from one that’s in Texas, and the model needs to recognize this and adjust for it. The same must 
be done for groups of evergreen trees throughout the country (or world), in order to truly be able to predict the type of tree anywhere. This model could also become more 
specific in the types of trees which it predicts, depending on if there’s a noticeable difference in the time series between different types of deciduous or evergreen trees. 
More research would need to be done to determine if this is feasible to do with NDVI data only.

This whole experience was a great learning process for me. I was able to garner some python experience with data science tools over the spring with my junior project, and I was 
able to expand and develop those skills even further through this project. As I coded more and more, I started finding myself debugging and solving any problems quicker and 
quicker as the project went on. Something that may have taken me hours to figure out at the start of this project may have only taken 30 minutes near the end. Specifically, I 
learned a great deal about the Pandas and NumPy packages through this project. Again, I used these tools already in the spring, but I was able to use them without constantly 
looking up different functions and ways to do things within these packages. A great example of this was the Pandas functions ‘iloc’ and ‘loc’, which I struggled to understand 
initially, but I later was able to use with ease to help me accomplish several things. 

For my junior project in the spring, I prepared my datasets before I imported them into Python, but this project taught me how to modify and prepare that data in the notebook 
itself, preserving the raw data and keeping everything in one place. Before Python, I may have calculated the rolling mean and standard deviation within Google Sheets and 
manually picked out outliers that way. I also got an overview of how a data science project normally operates. You take the data, prepare for the model by calculating any 
necessary parameters or columns, remove outliers and interpolate if necessary, then train and test your model. This is a process I’m sure I’ll repeat many times in the next 
several years, so it was great to get the underlying concepts to go along with the specifics. Through this project, I was able to also get an insight into how to quantify 
certain real-life things and classify them -- such as trees. Finally, this whole experience really gave me a sense of how things may operate in the workplace (to an extent). I 
had to pace and motivate myself on my own, and if there was really something I needed help with, or was confused about, I had to cooperate and work with my mentors and peer to 
figure it out. I learned a great deal about responsibility and how things truly operate in the real world, and for that I am grateful.
